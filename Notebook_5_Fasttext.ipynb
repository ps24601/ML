{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLeOu8Hp29n5"
   },
   "source": [
    "\n",
    "After having tried the previous models lets see the effect of n-grams embeddings (as discussed previosuly)\n",
    "[Fasttext](https://fasttext.cc/docs/en/english-vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqJZTwOK3kpH",
    "outputId": "c23b3473-9926-48f8-b0a7-1bbf24fb4e71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Lets mount the Google Drive and acccess the data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oj11yoeL3pMn"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "path = '/content/drive/MyDrive/IMDB/'  # path to the folder in google drive where data is saved\n",
    "with open(path + \"train.pkl\", 'rb') as a, open(path + \"test.pkl\", 'rb') as b:\n",
    "    d_train = pickle.load(a)   # loading training data\n",
    "    d_test = pickle.load(b)   # loading test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piEtsbAXJkFI",
    "outputId": "1ec438da-b766-400a-fe44-5cb546c89891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ktrain\n",
      "  Downloading ktrain-0.28.3.tar.gz (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 42.6 MB/s \n",
      "\u001b[?25hCollecting scikit-learn==0.23.2\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 49.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.5)\n",
      "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (21.3)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 37.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n",
      "Collecting cchardet\n",
      "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
      "\u001b[K     |████████████████████████████████| 263 kB 39.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.0.4)\n",
      "Collecting syntok\n",
      "  Downloading syntok-1.3.1.tar.gz (23 kB)\n",
      "Collecting seqeval==0.0.19\n",
      "  Downloading seqeval-0.0.19.tar.gz (30 kB)\n",
      "Collecting transformers<=4.10.3,>=4.0.0\n",
      "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 43.6 MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 46.0 MB/s \n",
      "\u001b[?25hCollecting keras_bert>=0.86.0\n",
      "  Downloading keras-bert-0.88.0.tar.gz (26 kB)\n",
      "Collecting whoosh\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[K     |████████████████████████████████| 468 kB 48.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (3.0.0)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.19->ktrain) (2.7.0)\n",
      "Collecting keras-transformer>=0.39.0\n",
      "  Downloading keras-transformer-0.39.0.tar.gz (11 kB)\n",
      "Collecting keras-pos-embd>=0.12.0\n",
      "  Downloading keras-pos-embd-0.12.0.tar.gz (6.0 kB)\n",
      "Collecting keras-multi-head>=0.28.0\n",
      "  Downloading keras-multi-head-0.28.0.tar.gz (14 kB)\n",
      "Collecting keras-layer-normalization>=0.15.0\n",
      "  Downloading keras-layer-normalization-0.15.0.tar.gz (4.2 kB)\n",
      "Collecting keras-position-wise-feed-forward>=0.7.0\n",
      "  Downloading keras-position-wise-feed-forward-0.7.0.tar.gz (4.5 kB)\n",
      "Collecting keras-embed-sim>=0.9.0\n",
      "  Downloading keras-embed-sim-0.9.0.tar.gz (4.1 kB)\n",
      "Collecting keras-self-attention>=0.50.0\n",
      "  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (3.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ktrain) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.3,>=4.0.0->ktrain) (4.8.2)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 44.1 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 313 kB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 33.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.3,>=4.0.0->ktrain) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.3,>=4.0.0->ktrain) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.3,>=4.0.0->ktrain) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 38.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers<=4.10.3,>=4.0.0->ktrain) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.10.3,>=4.0.0->ktrain) (3.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.24.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.10.3,>=4.0.0->ktrain) (7.1.2)\n",
      "Building wheels for collected packages: ktrain, seqeval, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, syntok\n",
      "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ktrain: filename=ktrain-0.28.3-py3-none-any.whl size=25292659 sha256=bbdebe84946462948ced446d208a60e1518f8661d2718e49cb953f398e2876f1\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/7e/c3/f46cdfc2b81c54424923b1405d7e670c35cacc11ada9a47b1c\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.19-py3-none-any.whl size=9929 sha256=88f4b8137c8650398fefd33297409013e5d9246d84b53ec8d7e11621d4104181\n",
      "  Stored in directory: /root/.cache/pip/wheels/f5/ac/f1/4e13d7aff05c722d142b7d20a88ad63f9aab11b895411241a4\n",
      "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-bert: filename=keras_bert-0.88.0-py3-none-any.whl size=34204 sha256=0686df825396972ebe3e1c17b1c45d2db404f36109846518c1f3e75363100913\n",
      "  Stored in directory: /root/.cache/pip/wheels/a2/90/cd/c038f2366929a3a5e3414a303b673e10235e802d871d29a835\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-transformer: filename=keras_transformer-0.39.0-py3-none-any.whl size=12842 sha256=2b69756deb2a56015b957c34cd448136a45576ba8a7211c8e5bb572446663131\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/01/e0/5a1a14bed6726f2ed73f7917d2d2c2d4081d2c88426dea07ce\n",
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.9.0-py3-none-any.whl size=4504 sha256=313bed474b1989b4a4813cce86fe687eea7bf59ae5dbebdd0c4dec8bfe9fe3b0\n",
      "  Stored in directory: /root/.cache/pip/wheels/a8/1e/d2/9bc15513dd2f8b9de3e628b3aa9d2de49e721deef6bbd1497e\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.15.0-py3-none-any.whl size=5224 sha256=db7f74b1c1364f56c495f4c3bada4e7baffcff505e9ee9c798b624111462f4d4\n",
      "  Stored in directory: /root/.cache/pip/wheels/4d/be/fe/55422f77ac11fe6ddcb471198038de8a26b5a4dd1557883c1e\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-multi-head: filename=keras_multi_head-0.28.0-py3-none-any.whl size=15559 sha256=93fae979962b14fc05f9cc17130c4efdb7379e1928f8d3ceb5fa2e917c2d11e0\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/4a/ea/9503ab5a02201dfb8635ba2cc8f30844661623c684a5b44472\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.12.0-py3-none-any.whl size=7469 sha256=fd360f88d28ca8b7e8c0e6b1290e74516b13a1778f7dcd144d3681fbbfa92510\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/99/fd/dd98f4876c3ebbef7aab0dbfbd37bca41d7db37d3a28b2cb09\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.7.0-py3-none-any.whl size=5541 sha256=d6112767c29b86f70c93b8cf98b06fa255180cae9e2c640f904839d924695a7d\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/12/02/1ad455c4f181cda1a4e60c5445855853d5c2ea91f942586a04\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19414 sha256=3ba18b70395cf77820bf4c4e28dd1a520d981191b621e2eee6fe79139a56e1dc\n",
      "  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=db1c04fec700c93878513478ace3a94c64352f53d8b74f96d11401b9a986e1be\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
      "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for syntok: filename=syntok-1.3.1-py3-none-any.whl size=20917 sha256=308a446ab81e2fb570006d9a257d255060a5b8e23c55a9fc8f8587f4c114de77\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/c2/33/e5d7d8f2f8b0c391d76bf82b844c3151bf23a84d75d02b185f\n",
      "Successfully built ktrain seqeval keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect syntok\n",
      "Installing collected packages: keras-self-attention, pyyaml, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, tokenizers, sacremoses, keras-transformer, huggingface-hub, whoosh, transformers, syntok, seqeval, sentencepiece, scikit-learn, langdetect, keras-bert, cchardet, ktrain\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.1\n",
      "    Uninstalling scikit-learn-1.0.1:\n",
      "      Successfully uninstalled scikit-learn-1.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\u001b[0m\n",
      "Successfully installed cchardet-2.1.7 huggingface-hub-0.2.1 keras-bert-0.88.0 keras-embed-sim-0.9.0 keras-layer-normalization-0.15.0 keras-multi-head-0.28.0 keras-pos-embd-0.12.0 keras-position-wise-feed-forward-0.7.0 keras-self-attention-0.50.0 keras-transformer-0.39.0 ktrain-0.28.3 langdetect-1.0.9 pyyaml-6.0 sacremoses-0.0.46 scikit-learn-0.23.2 sentencepiece-0.1.96 seqeval-0.0.19 syntok-1.3.1 tokenizers-0.10.3 transformers-4.10.3 whoosh-2.7.4\n"
     ]
    }
   ],
   "source": [
    "# we wil be using the ktrain for loading pretrained models and embedding for next steps as this takes care of preprcoessing automatically behind the curtain.\n",
    "# https://github.com/amaiya/ktrain\n",
    "\n",
    "! pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAO0Q9YPnfYz",
    "outputId": "e33af31a-3c22-4eda-e4c7-1eb9b3035229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[?25l\r\n",
      "\u001b[K     |▎                               | 10 kB 24.5 MB/s eta 0:00:01\r\n",
      "\u001b[K     |▋                               | 20 kB 30.0 MB/s eta 0:00:01\r\n",
      "\u001b[K     |▉                               | 30 kB 13.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█▏                              | 40 kB 9.9 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█▌                              | 51 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█▊                              | 61 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██                              | 71 kB 5.9 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██▍                             | 81 kB 6.6 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██▋                             | 92 kB 6.9 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███                             | 102 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███▎                            | 112 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███▌                            | 122 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███▉                            | 133 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████▏                           | 143 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████▍                           | 153 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████▊                           | 163 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████                           | 174 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████▎                          | 184 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████▋                          | 194 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████▉                          | 204 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████▏                         | 215 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████▌                         | 225 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████▊                         | 235 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████                         | 245 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████▍                        | 256 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████▋                        | 266 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████                        | 276 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████▎                       | 286 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████▌                       | 296 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████▉                       | 307 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████                       | 317 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████▍                      | 327 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████▊                      | 337 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████                      | 348 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████▎                     | 358 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████▋                     | 368 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████▉                     | 378 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████▏                    | 389 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████▌                    | 399 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████▊                    | 409 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████                    | 419 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████▍                   | 430 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████▋                   | 440 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████                   | 450 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████▎                  | 460 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████▌                  | 471 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████▉                  | 481 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████                  | 491 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████▍                 | 501 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████▊                 | 512 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████                 | 522 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████▎                | 532 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████▋                | 542 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████▉                | 552 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████▏               | 563 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████▌               | 573 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████▊               | 583 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████               | 593 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████▍              | 604 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████▋              | 614 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████              | 624 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████▏             | 634 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████▌             | 645 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████▉             | 655 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████             | 665 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████▍            | 675 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████▊            | 686 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████            | 696 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████▎           | 706 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████▋           | 716 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████▉           | 727 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████▏          | 737 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████▌          | 747 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████▊          | 757 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████          | 768 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████▎         | 778 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████▋         | 788 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████         | 798 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████▏        | 808 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████▌        | 819 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████▉        | 829 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████        | 839 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████▍       | 849 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████▊       | 860 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████████       | 870 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████████▎      | 880 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████████▋      | 890 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████████▉      | 901 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████▏     | 911 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████▌     | 921 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████▊     | 931 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████████     | 942 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████████▎    | 952 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████████▋    | 962 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████████    | 972 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████████▏   | 983 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████████▌   | 993 kB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████████▉   | 1.0 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████████████   | 1.0 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████████████▍  | 1.0 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████████  | 1.0 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████████▉ | 1.1 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████████████▏| 1.1 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████████████▍| 1.1 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████████████▊| 1.1 MB 5.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
      "Installing collected packages: tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.15.0\n"
     ]
    }
   ],
   "source": [
    "# for RADAM optimizer\n",
    "! pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6qzuDwiBnrAC"
   },
   "outputs": [],
   "source": [
    "# some packages and library we will need\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.optimizers import RectifiedAdam\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIYUew7pKSf-",
    "outputId": "89bcae50-f8e2-4bec-ed74-3e86375a03de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_label', 'label']\n",
      "       not_label  label\n",
      "6761         0.0    1.0\n",
      "24254        0.0    1.0\n",
      "12726        0.0    1.0\n",
      "21750        1.0    0.0\n",
      "1253         0.0    1.0\n",
      "['not_label', 'label']\n",
      "       not_label  label\n",
      "24887        1.0    0.0\n",
      "24727        1.0    0.0\n",
      "18830        0.0    1.0\n",
      "22855        1.0    0.0\n",
      "13498        0.0    1.0\n",
      "language: en\n",
      "Word Counts: 66940\n",
      "Nrows: 12500\n",
      "12500 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 227\n",
      "\t95percentile : 583\n",
      "\t99percentile : 887\n",
      "Adding 3-gram features\n",
      "max_features changed to 2671062 with addition of ngrams\n",
      "Average train sequence length with ngrams: 678\n",
      "train (w/ngrams) sequence lengths:\n",
      "\tmean : 679\n",
      "\t95percentile : 1746\n",
      "\t99percentile : 2658\n",
      "x_train shape: (12500,200)\n",
      "y_train shape: (12500, 2)\n",
      "Is Multi-Label? False\n",
      "12500 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 224\n",
      "\t95percentile : 571\n",
      "\t99percentile : 873\n",
      "Average test sequence length with ngrams: 497\n",
      "test (w/ngrams) sequence lengths:\n",
      "\tmean : 497\n",
      "\t95percentile : 1249\n",
      "\t99percentile : 1849\n",
      "x_test shape: (12500,200)\n",
      "y_test shape: (12500, 2)\n"
     ]
    }
   ],
   "source": [
    "# creating trianing and validation set from train set\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), preproc = ktrain.text.texts_from_df(train_df=d_train,\n",
    "                                                                   text_column = 'text',\n",
    "                                                                   label_columns = 'label',\n",
    "                                                                   ngram_range=3,\n",
    "                                                                  #  val_df = d_test,\n",
    "                                                                   val_pct = 0.5,  # keeeping training batch low for faster runtime\n",
    "                                                                   maxlen = 200,\n",
    "                                                                   preprocess_mode = 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dBvbWueK5ky",
    "outputId": "2e63c05c-ace4-40a9-f1c9-5a7207c0e25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 200\n",
      "done.\n",
      "simulating training for different learning rates... this may take a few moments...\n",
      "Epoch 1/1024\n",
      "782/782 [==============================] - 1270s 2s/step - loss: 0.9094 - accuracy: 0.5045\n",
      "Epoch 2/1024\n",
      "782/782 [==============================] - 1275s 2s/step - loss: 0.7564 - accuracy: 0.5162\n",
      "Epoch 3/1024\n",
      "782/782 [==============================] - 280s 357ms/step - loss: 636.6793 - accuracy: 0.4927\n",
      "\n",
      "\n",
      "done.\n",
      "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
     ]
    }
   ],
   "source": [
    "# model = text.text_classifier('fasttext', trn, preproc=preproc)\n",
    "\n",
    "model = text.text_classifier('fasttext',(x_train, y_train), preproc=preproc)\n",
    "learner = ktrain.get_learner(model, \n",
    "                             train_data=(x_train, y_train), \n",
    "                             val_data=(x_val, y_val), \n",
    "                             batch_size= 16)\n",
    "\n",
    "learner.lr_find()             # briefly simulate training to find good learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "MQjfNQA1Wzct",
    "outputId": "1c095bfd-1cd3-4a9c-b071-518eac7e6cc4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IfYWA5ljWU9"
   },
   "source": [
    "We can choose the learning rate in the the range of 0.005 to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aIsMJGXmjVP9",
    "outputId": "88c190a6-442b-4085-e7c0-d320e05196b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 200\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('fasttext',(x_train,y_train) , preproc=preproc)\n",
    "\n",
    "# param for early stopping, number of epoch to wait before terminating the process, if no improvmenet seen for the monitored quantity\n",
    "patience = 2\n",
    "\n",
    "# loss and metrics for the models\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy',   # as we are using binary accuracy which is same as accuracy for two category\n",
    "                                                  patience=patience,\n",
    "                                                  mode='max', restore_best_weights = False)\n",
    "\n",
    "model.compile(optimizer=RectifiedAdam(learning_rate = 0.01, warmup_proportion = 0.2, beta_1 = 0.9, \n",
    "                                           total_steps= 3000, weight_decay = 0.05, min_lr= 0.001),loss= loss, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "q1hWHxT1kVwa",
    "outputId": "3d25cf14-2f80-4944-d8e5-120a00265f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "782/782 [==============================] - 1746s 2s/step - loss: 0.6719 - binary_accuracy: 0.5786 - val_loss: 0.6061 - val_binary_accuracy: 0.7144\n",
      "Epoch 2/6\n",
      "782/782 [==============================] - 1724s 2s/step - loss: 0.4817 - binary_accuracy: 0.7726 - val_loss: 0.5130 - val_binary_accuracy: 0.7449\n",
      "Epoch 3/6\n",
      "782/782 [==============================] - 1711s 2s/step - loss: 0.3839 - binary_accuracy: 0.8323 - val_loss: 0.4788 - val_binary_accuracy: 0.7689\n",
      "Epoch 4/6\n",
      "782/782 [==============================] - 1714s 2s/step - loss: 0.2889 - binary_accuracy: 0.8860 - val_loss: 0.4761 - val_binary_accuracy: 0.7723\n",
      "Epoch 5/6\n",
      "197/782 [======>.......................] - ETA: 21:18 - loss: 0.2415 - binary_accuracy: 0.9055"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-89350533e82d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# n_cycles* cycle_len = total number of epochs, in each cycle, the optimizer lr gradually decays down.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_cycles\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, n_cycles, cycle_len, cycle_mult, lr_decay, checkpoint_folder, early_stopping, verbose, class_weight, callbacks, steps_per_epoch)\u001b[0m\n\u001b[1;32m   1180\u001b[0m                                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                   \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                                   callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgdr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finding epochs value ( Number fo epochs = 4). One can argue that we can keep the epoch value where val-accuracy is highest,\n",
    "# but it is advisable to go beyond that and chose epoch value when training and valid begin to diverge.\n",
    "\n",
    "learner = ktrain.get_learner(model, \n",
    "                             train_data=(x_train,y_train), \n",
    "                             val_data=(x_val,y_val), \n",
    "                             batch_size= 16)\n",
    "\n",
    "# https://github.com/amaiya/ktrain/blob/a871779581eea37c5e0211a3f787af8c7f2e9522/ktrain/core.py#L833\n",
    "# n_cycles* cycle_len = total number of epochs, in each cycle, the optimizer lr gradually decays down.\n",
    "\n",
    "learner.fit(lr = 1e-2,n_cycles= 3, cycle_len = 2, callbacks= [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVFGFpaRkbL0",
    "outputId": "94c68bae-8fb3-4226-8fee-c80069e16fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_label', 'label']\n",
      "   not_label  label\n",
      "0        1.0    0.0\n",
      "1        1.0    0.0\n",
      "2        1.0    0.0\n",
      "3        0.0    1.0\n",
      "4        0.0    1.0\n",
      "['not_label', 'label']\n",
      "   not_label  label\n",
      "0        0.0    1.0\n",
      "1        0.0    1.0\n",
      "2        1.0    0.0\n",
      "3        1.0    0.0\n",
      "4        0.0    1.0\n",
      "language: en\n",
      "Word Counts: 93788\n",
      "Nrows: 25000\n",
      "25000 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 226\n",
      "\t95percentile : 578\n",
      "\t99percentile : 881\n",
      "Adding 2-gram features\n",
      "max_features changed to 1234298 with addition of ngrams\n",
      "Average train sequence length with ngrams: 451\n",
      "train (w/ngrams) sequence lengths:\n",
      "\tmean : 451\n",
      "\t95percentile : 1155\n",
      "\t99percentile : 1761\n",
      "x_train shape: (25000,200)\n",
      "y_train shape: (25000, 2)\n",
      "Is Multi-Label? False\n",
      "25000 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 220\n",
      "\t95percentile : 556\n",
      "\t99percentile : 860\n",
      "Average test sequence length with ngrams: 405\n",
      "test (w/ngrams) sequence lengths:\n",
      "\tmean : 405\n",
      "\t95percentile : 1017\n",
      "\t99percentile : 1571\n",
      "x_test shape: (25000,200)\n",
      "y_test shape: (25000, 2)\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 200\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# creating finaldata set for maxlen = 200 \n",
    "(x_train, y_train), (x_val, y_val), preproc = ktrain.text.texts_from_df(train_df=d_train,\n",
    "                                                                   text_column = 'text',\n",
    "                                                                   label_columns = 'label',\n",
    "                                                                   ngram_range=2,\n",
    "                                                                   val_df = d_test,\n",
    "                                                                   maxlen = 200,\n",
    "                                                                   preprocess_mode = 'standard')\n",
    "\n",
    "model = text.text_classifier('fasttext',(x_train,y_train) , preproc=preproc)\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "model.compile(optimizer=RectifiedAdam(learning_rate = 0.005, warmup_proportion = 0.2, beta_1 = 0.9, \n",
    "                                           total_steps= 3000, weight_decay = 0.1, min_lr= 0.001),loss= loss, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9BOuGvSle_G",
    "outputId": "f0923525-626f-4f38-85cc-1bb6ecf7d47d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 0.005...\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 1762s 1s/step - loss: 0.7512 - binary_accuracy: 0.5352\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 1763s 1s/step - loss: 0.5879 - binary_accuracy: 0.6813\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 1755s 1s/step - loss: 0.4725 - binary_accuracy: 0.7738\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 1763s 1s/step - loss: 0.3861 - binary_accuracy: 0.8280\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 1766s 1s/step - loss: 0.3309 - binary_accuracy: 0.8580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d22b131d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = ktrain.get_learner(model, \n",
    "                             train_data=(x_train,y_train), \n",
    "                             batch_size= 16)\n",
    "learner.fit_onecycle(lr = 5e-3,epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETiOpiOUxb71",
    "outputId": "91a392b0-9233-4318-f138-4d59a709694c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 0.005...\n",
      "1563/1563 [==============================] - 1771s 1s/step - loss: 0.2912 - binary_accuracy: 0.8773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9cfd518350>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increasing one more epoch\n",
    "learner.fit_onecycle(lr = 5e-3,epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nnOj3G8O6O2U",
    "outputId": "2d0acecb-8d75-4436-c0ad-5af6e4c65c6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 0.001...\n",
      "1563/1563 [==============================] - 1762s 1s/step - loss: 0.2582 - binary_accuracy: 0.8963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d1a0c0350>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit_onecycle(lr = 1e-3,epochs = 1)\n",
    "# increasing one more epoch with max lr of 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fEg73C01bE2",
    "outputId": "02923ecd-8535-444d-af02-d4fc80435ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...FASTtext\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3370 - binary_accuracy: 0.8494\n",
      "loss=0.3370, accuracy: 84.9360%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evaluating on test set...{}\".format(\"FASTtext\"))\n",
    "(test_loss, test_accuracy) = learner.model.evaluate(x_val, y_val)\n",
    "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(test_loss,test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we gave away glove in trade off with n-grams approach, but there doesnt seem to be any improvement, in hindsight it seems to better include the n-grams in pre-trianed embeddings. However as we are not able to break the mark of 90% accuray lets turn to state-of-the art models ( Transformers)\n",
    "\n",
    "Note: Learning rate plot seems to suggest higher learning rate which cannot be right values, therefore fine tuning the leanring rate can substantially improve the measured metric."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Notebook_5_Fasttext.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
