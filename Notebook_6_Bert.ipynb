{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prashant P. Singh - 8448315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxjC0Hd7cDP8"
   },
   "source": [
    "As we have seen in case of embedding, there is huge amount of effort which goes in trianing the model and coming-up with representation matrix for different context. The good point is by using the Transfer learning approach we can use these pretrained embedding and make use of those. However what if the same can be said about the models. \n",
    "\n",
    "'Attention is all you need': Transformers are new buzz in the feild of Deeplearning and NLP and it is said the Transformer can revolutionize the field (its already happening). The contextual embedding combined with Transformer architecture was next way forward and right now there are models which are way superior than all rpevious avatars and models. However training such models from scratch is not possible and not advisible (until really needed and you have resource too). This transfer learning approach allows us to make use of model built by big companies like ( google, facebook etc) and use 'adapt' them to our need.\n",
    "\n",
    "For any pretrained model as has been highlighted by representation learning, the inner layer are the one which extract more basic feature and the outer layer are the ones which are building complex feature (thereby adapting to the task at hand) and are more prone to influence from the task we are doing. In this regard the best practice is to remove the last few layers, add your own Dense network and then train the model. \n",
    "\n",
    "Lets use the BERT (Bidirectional Encoder Representations  from Transformers, Google, 2018).[Link](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "Important library:'[Hugging face](https://huggingface.co/docs/transformers/training )' for diffrent kind of transofrmer based models with contextual embedding. Using k-train we can use the hugging face library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hp0R7n-Ef2l4",
    "outputId": "b10238b2-696f-46b9-9f7c-7ad4fb2e8425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 4.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
      "Installing collected packages: tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.15.0\n"
     ]
    }
   ],
   "source": [
    "# for RADAM optimizer\n",
    "! pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xG1LrXPzEx8",
    "outputId": "1a0848ea-6269-4cfd-c4a9-272b63f92246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ktrain\n",
      "  Downloading ktrain-0.28.3.tar.gz (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 1.6 MB/s \n",
      "\u001b[?25hCollecting scikit-learn==0.23.2\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 20.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.5)\n",
      "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (21.3)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 50.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n",
      "Collecting cchardet\n",
      "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
      "\u001b[K     |████████████████████████████████| 263 kB 51.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.0.4)\n",
      "Collecting syntok\n",
      "  Downloading syntok-1.3.1.tar.gz (23 kB)\n",
      "Collecting seqeval==0.0.19\n",
      "  Downloading seqeval-0.0.19.tar.gz (30 kB)\n",
      "Collecting transformers<=4.10.3,>=4.0.0\n",
      "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 39.5 MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 45.4 MB/s \n",
      "\u001b[?25hCollecting keras_bert>=0.86.0\n",
      "  Downloading keras-bert-0.88.0.tar.gz (26 kB)\n",
      "Collecting whoosh\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[K     |████████████████████████████████| 468 kB 48.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (3.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.19.5)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.19->ktrain) (2.7.0)\n",
      "Collecting keras-transformer>=0.39.0\n",
      "  Downloading keras-transformer-0.39.0.tar.gz (11 kB)\n",
      "Collecting keras-pos-embd>=0.12.0\n",
      "  Downloading keras-pos-embd-0.12.0.tar.gz (6.0 kB)\n",
      "Collecting keras-multi-head>=0.28.0\n",
      "  Downloading keras-multi-head-0.28.0.tar.gz (14 kB)\n",
      "Collecting keras-layer-normalization>=0.15.0\n",
      "  Downloading keras-layer-normalization-0.15.0.tar.gz (4.2 kB)\n",
      "Collecting keras-position-wise-feed-forward>=0.7.0\n",
      "  Downloading keras-position-wise-feed-forward-0.7.0.tar.gz (4.5 kB)\n",
      "Collecting keras-embed-sim>=0.9.0\n",
      "  Downloading keras-embed-sim-0.9.0.tar.gz (4.1 kB)\n",
      "Collecting keras-self-attention>=0.50.0\n",
      "  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (3.0.6)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ktrain) (1.15.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 39.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.3,>=4.0.0->ktrain) (4.62.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.3,>=4.0.0->ktrain) (3.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.3,>=4.0.0->ktrain) (4.8.2)\n",
      "Collecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 539 kB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.10.3,>=4.0.0->ktrain) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 47.6 MB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 52.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers<=4.10.3,>=4.0.0->ktrain) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.10.3,>=4.0.0->ktrain) (3.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2021.10.8)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.10.3,>=4.0.0->ktrain) (7.1.2)\n",
      "Building wheels for collected packages: ktrain, seqeval, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, syntok\n",
      "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ktrain: filename=ktrain-0.28.3-py3-none-any.whl size=25292659 sha256=00cd061a6538225119e4b6670c4fef39f4d9ddcc382d6191d1fcdc9be6761d7e\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/7e/c3/f46cdfc2b81c54424923b1405d7e670c35cacc11ada9a47b1c\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.19-py3-none-any.whl size=9929 sha256=b58ccda1e941e6bca774756ebf0dcc5b4cade3c394dd487db2083a718edb84f0\n",
      "  Stored in directory: /root/.cache/pip/wheels/f5/ac/f1/4e13d7aff05c722d142b7d20a88ad63f9aab11b895411241a4\n",
      "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-bert: filename=keras_bert-0.88.0-py3-none-any.whl size=34204 sha256=e4e4cb28f53462ada129f6b0e735fcad8565911d8e3a97d882d438f6778d5514\n",
      "  Stored in directory: /root/.cache/pip/wheels/a2/90/cd/c038f2366929a3a5e3414a303b673e10235e802d871d29a835\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-transformer: filename=keras_transformer-0.39.0-py3-none-any.whl size=12842 sha256=ed52f7aaeb39579431e93b33f3203b2963207ce7d9ee400ba1a0ab5657774beb\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/01/e0/5a1a14bed6726f2ed73f7917d2d2c2d4081d2c88426dea07ce\n",
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.9.0-py3-none-any.whl size=4504 sha256=c21ac7f3ee25c4a11173ce9cb393a670c7be1387a7b0339da58b5d3431ddb841\n",
      "  Stored in directory: /root/.cache/pip/wheels/a8/1e/d2/9bc15513dd2f8b9de3e628b3aa9d2de49e721deef6bbd1497e\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.15.0-py3-none-any.whl size=5224 sha256=090cacaba113564e37f09ddacedce2a5bd614f54eb4689aac804e9725fc2ddbc\n",
      "  Stored in directory: /root/.cache/pip/wheels/4d/be/fe/55422f77ac11fe6ddcb471198038de8a26b5a4dd1557883c1e\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-multi-head: filename=keras_multi_head-0.28.0-py3-none-any.whl size=15559 sha256=e44d06b4f626d49c7f79b87ba5f465793ee96345764da838d948df83aa80cd79\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/4a/ea/9503ab5a02201dfb8635ba2cc8f30844661623c684a5b44472\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.12.0-py3-none-any.whl size=7469 sha256=234ece9bf2ce6006f85eb88baa6f5249fc45f6b194514eb6c92207e4cb381f32\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/99/fd/dd98f4876c3ebbef7aab0dbfbd37bca41d7db37d3a28b2cb09\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.7.0-py3-none-any.whl size=5541 sha256=382ce9edefa86e596bad42782aa2d8f6e02dde83ec3e4d32b88ee241f1eb3290\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/12/02/1ad455c4f181cda1a4e60c5445855853d5c2ea91f942586a04\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19414 sha256=4a9c0e8fdd5abacacd7022bd1fbb8a3a72783309c5a36cd47a66caccc7bfff92\n",
      "  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=2a316d790bda7f2b5992e284bc178dc95ec4ba4e6b84d57deabdd5644d2db0bd\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
      "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for syntok: filename=syntok-1.3.1-py3-none-any.whl size=20917 sha256=f313ebabfa5ab301efe7c0c0f66ff494a88516bcd691ff1a4c88d8ca19f66fa2\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/c2/33/e5d7d8f2f8b0c391d76bf82b844c3151bf23a84d75d02b185f\n",
      "Successfully built ktrain seqeval keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect syntok\n",
      "Installing collected packages: keras-self-attention, pyyaml, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, tokenizers, sacremoses, keras-transformer, huggingface-hub, whoosh, transformers, syntok, seqeval, sentencepiece, scikit-learn, langdetect, keras-bert, cchardet, ktrain\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.1\n",
      "    Uninstalling scikit-learn-1.0.1:\n",
      "      Successfully uninstalled scikit-learn-1.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\u001b[0m\n",
      "Successfully installed cchardet-2.1.7 huggingface-hub-0.2.1 keras-bert-0.88.0 keras-embed-sim-0.9.0 keras-layer-normalization-0.15.0 keras-multi-head-0.28.0 keras-pos-embd-0.12.0 keras-position-wise-feed-forward-0.7.0 keras-self-attention-0.50.0 keras-transformer-0.39.0 ktrain-0.28.3 langdetect-1.0.9 pyyaml-6.0 sacremoses-0.0.46 scikit-learn-0.23.2 sentencepiece-0.1.96 seqeval-0.0.19 syntok-1.3.1 tokenizers-0.10.3 transformers-4.10.3 whoosh-2.7.4\n"
     ]
    }
   ],
   "source": [
    "# we wil be using the ktrain for loading pretrained models and embedding for next steps as this takes care of preprcoessing automatically behind the curtain.\n",
    "# https://github.com/amaiya/ktrain\n",
    "! pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b3ZITCQ4eIbZ"
   },
   "outputs": [],
   "source": [
    "# some packages and library we will need\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.optimizers import RectifiedAdam\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBpEO-EkhWje",
    "outputId": "e53997a9-ffb4-4481-dafe-0fc7d1564263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Lets mount the Google Drive and acccess the data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Bce3_DF8hdHv"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "path = '/content/drive/MyDrive/IMDB/'  # path to the folder in google drive where data is saved\n",
    "with open(path + \"train.pkl\", 'rb') as a, open(path + \"test.pkl\", 'rb') as b:\n",
    "    d_train = pickle.load(a)   # loading training data\n",
    "    d_test = pickle.load(b)   # loading test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PayGpXeixNBJ"
   },
   "outputs": [],
   "source": [
    "import ktrain\n",
    "from ktrain import text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1jMHWeZOLJF"
   },
   "outputs": [],
   "source": [
    "# creating trianing and validation set from train set\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), preproc = ktrain.text.texts_from_df(train_df=d_train,\n",
    "                                                                   text_column = 'text',\n",
    "                                                                   label_columns = 'label',\n",
    "                                                                   val_pct = 0.7,    # training size is kept so low to make the learner plot run faster\n",
    "                                                                   maxlen = 200,\n",
    "                                                                   preprocess_mode = 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sx2Nr1VUzZib",
    "outputId": "ee7b0425-4403-47e6-cf7b-7262f4268d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 200\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('bert',(x_train,y_train) , preproc=preproc)\n",
    "learner = ktrain.get_learner(model, \n",
    "                             train_data=(x_train,y_train), \n",
    "                             val_data=(x_val,y_val), \n",
    "                             batch_size= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAbh5GxVzzGk",
    "outputId": "c498f1ac-d89f-461d-b29b-cc6d722d3de9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating training for different learning rates... this may take a few moments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "2084/2084 [==============================] - 855s 398ms/step - loss: 3.3996 - accuracy: 0.5959\n",
      "\n",
      "\n",
      "done.\n",
      "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find()             # briefly simulate training to find good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "0T3zbKfCOkjO",
    "outputId": "d37057bb-fef8-430c-f32c-ba8ab9799183"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDSv-RSfO0sx"
   },
   "source": [
    "As can be seen above the good range of leanring rate is from 0.00005 to 0.0005. Therefore for this maxlen = 200 we can use the leanring rate with max value as 0.0002 and decaying down till the value of 0.00005. ( Radam is just exactly which can do this for us further it can take care of warm-up phase too.).  However we can make the change to maxlen and see what is the effect of same on learning rate ( which we will do next), for now let's try to find optimal value for number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BoKdIXuQY96X",
    "outputId": "e59a7251-40e0-4002-a08c-99aa4749cb65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 200\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('bert',(x_train,y_train) , preproc=preproc)\n",
    "\n",
    "# param for early stopping, number of epoch to wait before terminating the process, if no improvmenet seen for the monitored quantity\n",
    "patience = 2\n",
    "\n",
    "# loss and metrics for the models\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy',   # as we are using binary accuracy which is same as accuracy for two category\n",
    "                                                  patience=patience,\n",
    "                                                  mode='max', restore_best_weights = False)\n",
    "\n",
    "model.compile(optimizer=RectifiedAdam(learning_rate = 0.0001, warmup_proportion = 0.2, beta_1 = 0.9, \n",
    "                                           total_steps= 3000, weight_decay = 0.05, min_lr= 0.00002),loss= loss, metrics = metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCl9I5HJNAgs",
    "outputId": "ae78274c-b54c-45e6-9b5c-2fedab0b8ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1250/1250 [==============================] - 631s 485ms/step - loss: 0.3523 - binary_accuracy: 0.8374 - val_loss: 0.2476 - val_binary_accuracy: 0.9026\n",
      "Epoch 2/6\n",
      "1250/1250 [==============================] - 602s 482ms/step - loss: 0.1841 - binary_accuracy: 0.9288 - val_loss: 0.2311 - val_binary_accuracy: 0.9140\n",
      "Epoch 3/6\n",
      "1250/1250 [==============================] - 602s 482ms/step - loss: 0.0763 - binary_accuracy: 0.9743 - val_loss: 0.2975 - val_binary_accuracy: 0.9026\n",
      "Epoch 4/6\n",
      "1250/1250 [==============================] - 602s 481ms/step - loss: 0.0415 - binary_accuracy: 0.9876 - val_loss: 0.3187 - val_binary_accuracy: 0.9084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3a4e017dd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding epochs value ( Number fo epochs = 4). One can argue that we can keep the epoch value where val-accuracy is highest,\n",
    "# but it is advisable to go beyond that and chose epoch value when training and valid begin to diverge.\n",
    "\n",
    "learner = ktrain.get_learner(model, \n",
    "                             train_data=(x_train,y_train), \n",
    "                             val_data=(x_val,y_val), \n",
    "                             batch_size= 16)\n",
    "\n",
    "# https://github.com/amaiya/ktrain/blob/a871779581eea37c5e0211a3f787af8c7f2e9522/ktrain/core.py#L833\n",
    "# n_cycles* cycle_len = total number of epochs, in each cycle, the optimizer lr gradually decays down.\n",
    "\n",
    "learner.fit(lr = 1e-4,n_cycles= 3, cycle_len = 2, early_stopping= early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "HvA77P1t0plj",
    "outputId": "1d47b408-1563-4e9f-e328-7ef6d901a39e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8\n",
      "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
      "[██████████████████████████████████████████████████]\n",
      "extracting pretrained BERT model...\n",
      "done.\n",
      "\n",
      "cleanup downloaded zip...\n",
      "done.\n",
      "\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating trianing and validation set from train set for maxlen = 500 (previous was 200)\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), preproc = ktrain.text.texts_from_df(train_df=d_train,\n",
    "                                                                   text_column = 'text',\n",
    "                                                                   label_columns = 'label',\n",
    "                                                                   val_pct = 0.5,    # training size is kept so low to make the learner plot run faster\n",
    "                                                                   maxlen = 200,\n",
    "                                                                   preprocess_mode = 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uggs981x06VY",
    "outputId": "3eda3a4b-f822-4c67-d4a7-c2b78b7acb93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 500\n",
      "done.\n",
      "simulating training for different learning rates... this may take a few moments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "2084/2084 [==============================] - 2309s 1s/step - loss: 3.4755 - accuracy: 0.6152\n",
      "\n",
      "\n",
      "done.\n",
      "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('bert',train_data, preproc=preproc)\n",
    "learner = ktrain.get_learner(model, \n",
    "                             train_data=train_data, \n",
    "                             val_data=val_data, \n",
    "                             batch_size= 6)\n",
    "\n",
    "# briefly simulate training to find good learning rate\n",
    "learner.lr_find()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "DWc4_TWM1GOV",
    "outputId": "7d0ee88b-2373-4ce4-d810-012114eda493"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meqMh1NBQzry"
   },
   "source": [
    "Though the graph is similar to previous but we can by looking say that the range of leanring rate is max = 0.0002 and min = 0.00008. We will use these values in Radam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eEyBWaYFq24",
    "outputId": "f39ada90-81a8-4e58-afa9-ec827064532f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 500\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('bert',(x_train,y_train) , preproc=preproc)\n",
    "patience = 2\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy',\n",
    "                                                  patience=patience,\n",
    "                                                  mode='max', restore_best_weights = False)\n",
    "\n",
    "# model = create_model(embedding_matrix= embedding_matrix, embedding_size= embedding_size, lstm_cell_size= 128)\n",
    "model.compile(optimizer=RectifiedAdam(learning_rate = 0.00008, warmup_proportion = 0.2, beta_1 = 0.9, \n",
    "                                           total_steps= 3000, weight_decay = 0.05, min_lr= 0.00002),loss= loss, metrics = metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ss1gsRhTGNO1",
    "outputId": "297d2d5f-1b5e-40ec-d921-7ab3c6a96c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "3334/3334 [==============================] - 1856s 548ms/step - loss: 0.2883 - binary_accuracy: 0.8744 - val_loss: 0.1845 - val_binary_accuracy: 0.9268\n",
      "Epoch 2/8\n",
      "3334/3334 [==============================] - 1824s 547ms/step - loss: 0.1302 - binary_accuracy: 0.9553 - val_loss: 0.3328 - val_binary_accuracy: 0.9014\n",
      "Epoch 3/8\n",
      "3334/3334 [==============================] - 1823s 547ms/step - loss: 0.0668 - binary_accuracy: 0.9776 - val_loss: 0.2504 - val_binary_accuracy: 0.9208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3667f70f10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding epochs value ( Number fo epochs = 4). One can argue that we can keep the epoch value where val-accuracy is highest,\n",
    "# but it is advisable to go beyond that and chose epoch value when training and valid begin to diverge.\n",
    "\n",
    "learner = ktrain.get_learner(model, \n",
    "                             train_data=(x_train,y_train), \n",
    "                             val_data=(x_val,y_val), \n",
    "                             batch_size= 6)\n",
    "learner.fit(lr = 1e-4,n_cycles= 4, cycle_len = 2, callbacks= [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YC8ZolgjXwBH"
   },
   "source": [
    "We can now make final run on the data set for amxlen = 200 and maxlen = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "phQhZl2NpQtQ",
    "outputId": "0a8e93d7-295f-49bb-c4c5-000e0c3b233f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_label', 'label']\n",
      "   not_label  label\n",
      "0        1.0    0.0\n",
      "1        1.0    0.0\n",
      "2        1.0    0.0\n",
      "3        0.0    1.0\n",
      "4        0.0    1.0\n",
      "['not_label', 'label']\n",
      "   not_label  label\n",
      "0        0.0    1.0\n",
      "1        0.0    1.0\n",
      "2        1.0    0.0\n",
      "3        1.0    0.0\n",
      "4        0.0    1.0\n",
      "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
      "[██████████████████████████████████████████████████]\n",
      "extracting pretrained BERT model...\n",
      "done.\n",
      "\n",
      "cleanup downloaded zip...\n",
      "done.\n",
      "\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating data set for maxlen = 200\n",
    "(x_train, y_train), (x_val, y_val), preproc = ktrain.text.texts_from_df(train_df=d_train,\n",
    "                                                                   text_column = 'text',\n",
    "                                                                   label_columns = 'label',\n",
    "                                                                   val_df = d_test,    # using test data now\n",
    "                                                                   maxlen = 200,\n",
    "                                                                   preprocess_mode = 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5tSDXlhy_4d",
    "outputId": "961591f6-1de2-4afe-bde4-16a811586b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 200\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('bert',(x_train,y_train) , preproc=preproc)\n",
    "patience = 2\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "model.compile(optimizer=RectifiedAdam(learning_rate = 0.0001, warmup_proportion = 0.2, beta_1 = 0.9, \n",
    "                                           total_steps= 3000, weight_decay = 0.1, min_lr= 0.00002),loss= loss, metrics = metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqNe7RGP0rfR",
    "outputId": "c09d6ce2-9b17-41dd-d975-c4762a5ef66d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 0.0001...\n",
      "Epoch 1/4\n",
      "1563/1563 [==============================] - 757s 459ms/step - loss: 0.3739 - binary_accuracy: 0.8228\n",
      "Epoch 2/4\n",
      "1563/1563 [==============================] - 717s 459ms/step - loss: 0.1956 - binary_accuracy: 0.9234\n",
      "Epoch 3/4\n",
      "1563/1563 [==============================] - 716s 458ms/step - loss: 0.0827 - binary_accuracy: 0.9735\n",
      "Epoch 4/4\n",
      "1563/1563 [==============================] - 717s 458ms/step - loss: 0.0484 - binary_accuracy: 0.9852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f98d2bdc310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "learner = ktrain.get_learner(model, \n",
    "                             train_data=(x_train,y_train), \n",
    "                             batch_size= 16)\n",
    "                              \n",
    "learner.fit_onecycle(lr = 1e-4,epochs = 4)   # epoch value of 4 is determined from previous run using early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "bKYcb4D7A85i",
    "outputId": "ff907f97-acf0-4c00-b5a6-1395926adefa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocessing the test dataset using the transform object 'preproc'\n",
    "x_test, y_test = preproc.preprocess_test(d_test.text, y = d_test.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FdL8R1sfD2zS"
   },
   "outputs": [],
   "source": [
    "# book keeping\n",
    "import pandas as pd\n",
    "report = pd.DataFrame(columns = ['model','Trail_loss','Train_accuracy', 'Test_loss','Test_accuracy']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PSknXMEDorG",
    "outputId": "c1cbfcdd-8bea-423b-cdde-40cc02f5a565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on valid set...200d_lr_0.0001\n",
      "782/782 [==============================] - 210s 262ms/step - loss: 0.0168 - binary_accuracy: 0.9971\n",
      "loss=0.0168, accuracy: 99.7080%\n",
      "Evaluating on test set...200d_lr_0.0001\n",
      "782/782 [==============================] - 205s 262ms/step - loss: 0.2811 - binary_accuracy: 0.9121\n",
      "loss=0.2811, accuracy: 91.2080%\n",
      "            model  Trail_loss  Train_accuracy  Test_loss  Test_accuracy\n",
      "0  200d_lr_0.0001    0.016768         0.99708   0.281085        0.91208\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data for maxlen = 200, lr = 0.0001\n",
    "\n",
    "name = '200d_lr_0.0001'\n",
    "print(\"Evaluating on valid set...{}\".format(name))\n",
    "(val_loss, val_accuracy) = learner.model.evaluate(x_train, y_train)\n",
    "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(val_loss,val_accuracy * 100))\n",
    "print(\"Evaluating on test set...{}\".format(name))\n",
    "(test_loss, test_accuracy) = learner.model.evaluate(x_test, y_test)\n",
    "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(test_loss,test_accuracy * 100))\n",
    "report.loc[len(report)] = [name,val_loss, val_accuracy, test_loss, test_accuracy]\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "CnRp-9emFfJg",
    "outputId": "fa815495-741d-4579-ebc3-ccf7e0b1f9ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_label', 'label']\n",
      "       not_label  label\n",
      "4156         0.0    1.0\n",
      "18325        1.0    0.0\n",
      "18918        0.0    1.0\n",
      "18063        0.0    1.0\n",
      "22337        1.0    0.0\n",
      "['not_label', 'label']\n",
      "       not_label  label\n",
      "22540        0.0    1.0\n",
      "11155        0.0    1.0\n",
      "239          1.0    0.0\n",
      "14311        1.0    0.0\n",
      "9243         1.0    0.0\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# running the model for maxlen = 500\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), preproc = ktrain.text.texts_from_df(train_df=d_train,\n",
    "                                                                   text_column = 'text',\n",
    "                                                                   label_columns = 'label',\n",
    "                                                                   val_pct = 0.2,\n",
    "                                                                   maxlen = 500,\n",
    "                                                                   preprocess_mode = 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fONrAHSjGs0U",
    "outputId": "42961b12-bc39-4adf-c0ae-c1a5739e6d25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 500\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('bert',(x_train,y_train) , preproc=preproc)\n",
    "patience = 2\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "model.compile(optimizer=RectifiedAdam(learning_rate = 0.00008, warmup_proportion = 0.2, beta_1 = 0.9, \n",
    "                                           total_steps= 3000, weight_decay = 0.05, min_lr= 0.00002),loss= loss, metrics = metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFXVnNqnG-xQ",
    "outputId": "ff8b334a-3677-4856-c2e6-3b9dda7c264c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 8e-05...\n",
      "Epoch 1/3\n",
      "4167/4167 [==============================] - 2192s 517ms/step - loss: 0.2933 - binary_accuracy: 0.8655\n",
      "Epoch 2/3\n",
      "4167/4167 [==============================] - 2150s 516ms/step - loss: 0.1415 - binary_accuracy: 0.9502\n",
      "Epoch 3/3\n",
      "4167/4167 [==============================] - 2150s 516ms/step - loss: 0.0757 - binary_accuracy: 0.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f984aeebe50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit for maxlen = 500\n",
    "learner = ktrain.get_learner(model, \n",
    "                             train_data=(x_train,y_train), \n",
    "                             batch_size= 6)\n",
    "learner.fit_onecycle(lr = 8e-5,epochs = 3)  # epoch value of 3 determined from previous run using early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "jNpiqfNzghku",
    "outputId": "20a8f6c6-d3c5-4969-b2e7-6251725f5326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on valid set...500d_lr_0.00008\n",
      "782/782 [==============================] - 566s 724ms/step - loss: 0.0313 - binary_accuracy: 0.9928\n",
      "loss=0.0313, accuracy: 99.2800%\n",
      "Evaluating on test set...500d_lr_0.00008\n",
      "782/782 [==============================] - 565s 723ms/step - loss: 0.1898 - binary_accuracy: 0.9318\n",
      "loss=0.1898, accuracy: 93.1800%\n",
      "             model  Trail_loss  Train_accuracy  Test_loss  Test_accuracy\n",
      "0   200d_lr_0.0001    0.016768         0.99708   0.281085        0.91208\n",
      "1  500d_lr_0.00008    0.031273         0.99280   0.189808        0.93180\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = preproc.preprocess_test(d_test.text, y = d_test.label)\n",
    "name = '500d_lr_0.00008'\n",
    "print(\"Evaluating on valid set...{}\".format(name))\n",
    "(val_loss, val_accuracy) = learner.model.evaluate(x_train, y_train)\n",
    "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(val_loss,val_accuracy * 100))\n",
    "print(\"Evaluating on test set...{}\".format(name))\n",
    "(test_loss, test_accuracy) = learner.model.evaluate(x_test, y_test)\n",
    "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(test_loss,test_accuracy * 100))\n",
    "report.loc[len(report)] = [name,val_loss, val_accuracy, test_loss, test_accuracy]\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "z8T7OroJnXYs",
    "outputId": "63a45b1a-d6ed-48b7-d49e-dbd9027f1dd5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Trail_loss</th>\n",
       "      <th>Train_accuracy</th>\n",
       "      <th>Test_loss</th>\n",
       "      <th>Test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200d_lr_0.0001</td>\n",
       "      <td>0.016768</td>\n",
       "      <td>0.99708</td>\n",
       "      <td>0.281085</td>\n",
       "      <td>0.91208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500d_lr_0.00008</td>\n",
       "      <td>0.031273</td>\n",
       "      <td>0.99280</td>\n",
       "      <td>0.189808</td>\n",
       "      <td>0.93180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  Trail_loss  Train_accuracy  Test_loss  Test_accuracy\n",
       "0   200d_lr_0.0001    0.016768         0.99708   0.281085        0.91208\n",
       "1  500d_lr_0.00008    0.031273         0.99280   0.189808        0.93180"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_TO8bAva-1O"
   },
   "source": [
    "As cen be seen both the models perform above 90% which was the mark acheived by our SVC model. the model with maxlen = 500 is substantially over the mark than SVC and that too using only 20k vocab size ( max_features), hence this can be improved upon further by fine tuning max_features, learning rate (further fine tuning) etc.\n",
    "\n",
    "Refrences:\n",
    "\n",
    "https://github.com/amaiya/ktrain/blob/master/examples/text/IMDb-BERT.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Notebook_6_Bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
